stages:
  - generate
  - benchmarks
  - macrobenchmarks
  - test-apps

variables:
  # This base image is created here: https://gitlab.ddbuild.io/DataDog/apm-reliability/benchmarking-platform/-/pipelines/56135449
  BASE_CI_IMAGE: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/benchmarking-platform:dd-trace-go-56135449
  INDEX_FILE: index.txt
  KUBERNETES_SERVICE_ACCOUNT_OVERWRITE: dd-trace-go
  FF_USE_LEGACY_KUBERNETES_EXECUTION_STRATEGY: "true"
  BENCHMARK_TARGETS: "BenchmarkStartRequestSpan|BenchmarkHttpServeTrace|BenchmarkTracerAddSpans|BenchmarkStartSpan|BenchmarkSingleSpanRetention|BenchmarkOTelApiWithCustomTags|BenchmarkInjectW3C|BenchmarkExtractW3C|BenchmarkPartialFlushing|BenchmarkConfig|BenchmarkStartSpanConfig|BenchmarkGraphQL|BenchmarkSampleWAFContext|BenchmarkCaptureStackTrace|BenchmarkSetTagString|BenchmarkSetTagStringPtr|BenchmarkSetTagMetric|BenchmarkSetTagStringer|BenchmarkSerializeSpanLinksInMeta|BenchmarkLogs|BenchmarkParallelLogs|BenchmarkMetrics|BenchmarkParallelMetrics"

# In order to run benchmarks in parallel, we generate a matrix of test names based on the BENCHMARK_TARGETS variable.
# This will be used in tandem with bp-runner in benchmarks.yml.
# This will allow us to spin up a child job in GitLab CI that handles running all of the benchmarks in parallel.
generate_matrix:
  stage: generate
  image: $BASE_CI_IMAGE
  tags: ["runner:apm-k8s-tweaked-metal"]
  script: |
    cd .gitlab
    go run generate_config.go
    mv generated_benchmark_matrix.yml ../generated_benchmark_matrix.yml
  artifacts:
    paths:
      - generated_benchmark_matrix.yml
    expire_in: 1 hour # Artifact is temporary, needed only for the current pipeline

trigger_child_pipeline:
  stage: benchmarks
  trigger:
    include:
      - artifact: generated_benchmark_matrix.yml
        job: generate_matrix
    strategy: depend
  needs:
    - generate_matrix

include:
  - local: ".gitlab/macrobenchmarks.yml"
  - local: ".gitlab/test-apps.yml"
